// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.5
//   protoc               v5.29.3
// source: opentelemetry/proto/profiles/v1development/profiles.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { InstrumentationScope, KeyValue } from "../../common/v1/common.ts";
import { Resource } from "../../resource/v1/resource.ts";

export const protobufPackage = "opentelemetry.proto.profiles.v1development";

/**
 * Specifies the method of aggregating metric values, either DELTA (change since last report)
 * or CUMULATIVE (total since a fixed start time).
 */
export enum AggregationTemporality {
  /** AGGREGATION_TEMPORALITY_UNSPECIFIED - UNSPECIFIED is the default AggregationTemporality, it MUST not be used. */
  AGGREGATION_TEMPORALITY_UNSPECIFIED = 0,
  /**
   * AGGREGATION_TEMPORALITY_DELTA - DELTA is an AggregationTemporality for a profiler which reports
   * changes since last report time. Successive metrics contain aggregation of
   * values from continuous and non-overlapping intervals.
   *
   * The values for a DELTA metric are based only on the time interval
   * associated with one measurement cycle. There is no dependency on
   * previous measurements like is the case for CUMULATIVE metrics.
   *
   * For example, consider a system measuring the number of requests that
   * it receives and reports the sum of these requests every second as a
   * DELTA metric:
   *
   * 1. The system starts receiving at time=t_0.
   * 2. A request is received, the system measures 1 request.
   * 3. A request is received, the system measures 1 request.
   * 4. A request is received, the system measures 1 request.
   * 5. The 1 second collection cycle ends. A metric is exported for the
   * number of requests received over the interval of time t_0 to
   * t_0+1 with a value of 3.
   * 6. A request is received, the system measures 1 request.
   * 7. A request is received, the system measures 1 request.
   * 8. The 1 second collection cycle ends. A metric is exported for the
   * number of requests received over the interval of time t_0+1 to
   * t_0+2 with a value of 2.
   */
  AGGREGATION_TEMPORALITY_DELTA = 1,
  /**
   * AGGREGATION_TEMPORALITY_CUMULATIVE - CUMULATIVE is an AggregationTemporality for a profiler which
   * reports changes since a fixed start time. This means that current values
   * of a CUMULATIVE metric depend on all previous measurements since the
   * start time. Because of this, the sender is required to retain this state
   * in some form. If this state is lost or invalidated, the CUMULATIVE metric
   * values MUST be reset and a new fixed start time following the last
   * reported measurement time sent MUST be used.
   *
   * For example, consider a system measuring the number of requests that
   * it receives and reports the sum of these requests every second as a
   * CUMULATIVE metric:
   *
   * 1. The system starts receiving at time=t_0.
   * 2. A request is received, the system measures 1 request.
   * 3. A request is received, the system measures 1 request.
   * 4. A request is received, the system measures 1 request.
   * 5. The 1 second collection cycle ends. A metric is exported for the
   * number of requests received over the interval of time t_0 to
   * t_0+1 with a value of 3.
   * 6. A request is received, the system measures 1 request.
   * 7. A request is received, the system measures 1 request.
   * 8. The 1 second collection cycle ends. A metric is exported for the
   * number of requests received over the interval of time t_0 to
   * t_0+2 with a value of 5.
   * 9. The system experiences a fault and loses state.
   * 10. The system recovers and resumes receiving at time=t_1.
   * 11. A request is received, the system measures 1 request.
   * 12. The 1 second collection cycle ends. A metric is exported for the
   * number of requests received over the interval of time t_1 to
   * t_1+1 with a value of 1.
   *
   * Note: Even though, when reporting changes since last report time, using
   * CUMULATIVE is valid, it is not recommended.
   */
  AGGREGATION_TEMPORALITY_CUMULATIVE = 2,
  UNRECOGNIZED = -1,
}

export function aggregationTemporalityFromJSON(object: any): AggregationTemporality {
  switch (object) {
    case 0:
    case "AGGREGATION_TEMPORALITY_UNSPECIFIED":
      return AggregationTemporality.AGGREGATION_TEMPORALITY_UNSPECIFIED;
    case 1:
    case "AGGREGATION_TEMPORALITY_DELTA":
      return AggregationTemporality.AGGREGATION_TEMPORALITY_DELTA;
    case 2:
    case "AGGREGATION_TEMPORALITY_CUMULATIVE":
      return AggregationTemporality.AGGREGATION_TEMPORALITY_CUMULATIVE;
    case -1:
    case "UNRECOGNIZED":
    default:
      return AggregationTemporality.UNRECOGNIZED;
  }
}

export function aggregationTemporalityToJSON(object: AggregationTemporality): string {
  switch (object) {
    case AggregationTemporality.AGGREGATION_TEMPORALITY_UNSPECIFIED:
      return "AGGREGATION_TEMPORALITY_UNSPECIFIED";
    case AggregationTemporality.AGGREGATION_TEMPORALITY_DELTA:
      return "AGGREGATION_TEMPORALITY_DELTA";
    case AggregationTemporality.AGGREGATION_TEMPORALITY_CUMULATIVE:
      return "AGGREGATION_TEMPORALITY_CUMULATIVE";
    case AggregationTemporality.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * ProfilesDictionary represents the profiles data shared across the
 * entire message being sent.
 */
export interface ProfilesDictionary {
  /**
   * Mappings from address ranges to the image/binary/library mapped
   * into that address range referenced by locations via Location.mapping_index.
   * mapping_table[0] must always be set to a zero value default mapping,
   * so that _index fields can use 0 to indicate null/unset.
   */
  mappingTable: Mapping[];
  /** Locations referenced by samples via Profile.location_indices. */
  locationTable: Location[];
  /** Functions referenced by locations via Line.function_index. */
  functionTable: FunctionMessage[];
  /**
   * Links referenced by samples via Sample.link_index.
   * link_table[0] must always be set to a zero value default link,
   * so that _index fields can use 0 to indicate null/unset.
   */
  linkTable: Link[];
  /**
   * A common table for strings referenced by various messages.
   * string_table[0] must always be "".
   */
  stringTable: string[];
  /** A common table for attributes referenced by various messages. */
  attributeTable: KeyValue[];
  /** Represents a mapping between Attribute Keys and Units. */
  attributeUnits: AttributeUnit[];
}

/**
 * ProfilesData represents the profiles data that can be stored in persistent storage,
 * OR can be embedded by other protocols that transfer OTLP profiles data but do not
 * implement the OTLP protocol.
 *
 * The main difference between this message and collector protocol is that
 * in this message there will not be any "control" or "metadata" specific to
 * OTLP protocol.
 *
 * When new fields are added into this message, the OTLP request MUST be updated
 * as well.
 */
export interface ProfilesData {
  /**
   * An array of ResourceProfiles.
   * For data coming from an SDK profiler, this array will typically contain one
   * element. Host-level profilers will usually create one ResourceProfile per
   * container, as well as one additional ResourceProfile grouping all samples
   * from non-containerized processes.
   * Other resource groupings are possible as well and clarified via
   * Resource.attributes and semantic conventions.
   */
  resourceProfiles: ResourceProfiles[];
  /** One instance of ProfilesDictionary */
  dictionary?: ProfilesDictionary | undefined;
}

/** A collection of ScopeProfiles from a Resource. */
export interface ResourceProfiles {
  /**
   * The resource for the profiles in this message.
   * If this field is not set then no resource info is known.
   */
  resource?:
    | Resource
    | undefined;
  /** A list of ScopeProfiles that originate from a resource. */
  scopeProfiles: ScopeProfiles[];
  /**
   * The Schema URL, if known. This is the identifier of the Schema that the resource data
   * is recorded in. Notably, the last part of the URL path is the version number of the
   * schema: http[s]://server[:port]/path/<version>. To learn more about Schema URL see
   * https://opentelemetry.io/docs/specs/otel/schemas/#schema-url
   * This schema_url applies to the data in the "resource" field. It does not apply
   * to the data in the "scope_profiles" field which have their own schema_url field.
   */
  schemaUrl: string;
}

/** A collection of Profiles produced by an InstrumentationScope. */
export interface ScopeProfiles {
  /**
   * The instrumentation scope information for the profiles in this message.
   * Semantically when InstrumentationScope isn't set, it is equivalent with
   * an empty instrumentation scope name (unknown).
   */
  scope?:
    | InstrumentationScope
    | undefined;
  /** A list of Profiles that originate from an instrumentation scope. */
  profiles: Profile[];
  /**
   * The Schema URL, if known. This is the identifier of the Schema that the profile data
   * is recorded in. Notably, the last part of the URL path is the version number of the
   * schema: http[s]://server[:port]/path/<version>. To learn more about Schema URL see
   * https://opentelemetry.io/docs/specs/otel/schemas/#schema-url
   * This schema_url applies to all profiles in the "profiles" field.
   */
  schemaUrl: string;
}

/**
 * Represents a complete profile, including sample types, samples,
 * mappings to binaries, locations, functions, string table, and additional metadata.
 * It modifies and annotates pprof Profile with OpenTelemetry specific fields.
 *
 * Note that whilst fields in this message retain the name and field id from pprof in most cases
 * for ease of understanding data migration, it is not intended that pprof:Profile and
 * OpenTelemetry:Profile encoding be wire compatible.
 */
export interface Profile {
  /**
   * A description of the samples associated with each Sample.value.
   * For a cpu profile this might be:
   *   [["cpu","nanoseconds"]] or [["wall","seconds"]] or [["syscall","count"]]
   * For a heap profile, this might be:
   *   [["allocations","count"], ["space","bytes"]],
   * If one of the values represents the number of events represented
   * by the sample, by convention it should be at index 0 and use
   * sample_type.unit == "count".
   */
  sampleType: ValueType[];
  /** The set of samples recorded in this profile. */
  sample: Sample[];
  /** References to locations in ProfilesDictionary.location_table. */
  locationIndices: number[];
  /** Time of collection (UTC) represented as nanoseconds past the epoch. */
  timeNanos: bigint;
  /** Duration of the profile, if a duration makes sense. */
  durationNanos: bigint;
  /**
   * The kind of events between sampled occurrences.
   * e.g [ "cpu","cycles" ] or [ "heap","bytes" ]
   */
  periodType?:
    | ValueType
    | undefined;
  /** The number of events between sampled occurrences. */
  period: bigint;
  /**
   * Free-form text associated with the profile. The text is displayed as is
   * to the user by the tools that read profiles (e.g. by pprof). This field
   * should not be used to store any machine-readable information, it is only
   * for human-friendly content. The profile must stay functional if this field
   * is cleaned.
   */
  commentStrindices: number[];
  /** Index into the sample_type array to the default sample type. */
  defaultSampleTypeIndex: number;
  /**
   * A globally unique identifier for a profile. The ID is a 16-byte array. An ID with
   * all zeroes is considered invalid.
   *
   * This field is required.
   */
  profileId: Uint8Array;
  /**
   * dropped_attributes_count is the number of attributes that were discarded. Attributes
   * can be discarded because their keys are too long or because there are too many
   * attributes. If this value is 0, then no attributes were dropped.
   */
  droppedAttributesCount: number;
  /** Specifies format of the original payload. Common values are defined in semantic conventions. [required if original_payload is present] */
  originalPayloadFormat: string;
  /**
   * Original payload can be stored in this field. This can be useful for users who want to get the original payload.
   * Formats such as JFR are highly extensible and can contain more information than what is defined in this spec.
   * Inclusion of original payload should be configurable by the user. Default behavior should be to not include the original payload.
   * If the original payload is in pprof format, it SHOULD not be included in this field.
   * The field is optional, however if it is present then equivalent converted data should be populated in other fields
   * of this message as far as is practicable.
   */
  originalPayload: Uint8Array;
  /**
   * References to attributes in attribute_table. [optional]
   * It is a collection of key/value pairs. Note, global attributes
   * like server name can be set using the resource API. Examples of attributes:
   *
   *     "/http/user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36"
   *     "/http/server_latency": 300
   *     "abc.com/myattribute": true
   *     "abc.com/score": 10.239
   *
   * The OpenTelemetry API specification further restricts the allowed value types:
   * https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/common/README.md#attribute
   * Attribute keys MUST be unique (it is not allowed to have more than one
   * attribute with the same key).
   */
  attributeIndices: number[];
}

/** Represents a mapping between Attribute Keys and Units. */
export interface AttributeUnit {
  /** Index into string table. */
  attributeKeyStrindex: number;
  /** Index into string table. */
  unitStrindex: number;
}

/**
 * A pointer from a profile Sample to a trace Span.
 * Connects a profile sample to a trace span, identified by unique trace and span IDs.
 */
export interface Link {
  /**
   * A unique identifier of a trace that this linked span is part of. The ID is a
   * 16-byte array.
   */
  traceId: Uint8Array;
  /** A unique identifier for the linked span. The ID is an 8-byte array. */
  spanId: Uint8Array;
}

/** ValueType describes the type and units of a value, with an optional aggregation temporality. */
export interface ValueType {
  /** Index into ProfilesDictionary.string_table. */
  typeStrindex: number;
  /** Index into ProfilesDictionary.string_table. */
  unitStrindex: number;
  aggregationTemporality: AggregationTemporality;
}

/**
 * Each Sample records values encountered in some program
 * context. The program context is typically a stack trace, perhaps
 * augmented with auxiliary information like the thread-id, some
 * indicator of a higher level request being handled etc.
 */
export interface Sample {
  /** locations_start_index along with locations_length refers to to a slice of locations in Profile.location_indices. */
  locationsStartIndex: number;
  /**
   * locations_length along with locations_start_index refers to a slice of locations in Profile.location_indices.
   * Supersedes location_index.
   */
  locationsLength: number;
  /**
   * The type and unit of each value is defined by the corresponding
   * entry in Profile.sample_type. All samples must have the same
   * number of values, the same as the length of Profile.sample_type.
   * When aggregating multiple samples into a single sample, the
   * result has a list of values that is the element-wise sum of the
   * lists of the originals.
   */
  value: bigint[];
  /** References to attributes in ProfilesDictionary.attribute_table. [optional] */
  attributeIndices: number[];
  /**
   * Reference to link in ProfilesDictionary.link_table. [optional]
   * It can be unset / set to 0 if no link exists, as link_table[0] is always a 'null' default value.
   */
  linkIndex: number;
  /**
   * Timestamps associated with Sample represented in nanoseconds. These timestamps are expected
   * to fall within the Profile's time range. [optional]
   */
  timestampsUnixNano: bigint[];
}

/**
 * Describes the mapping of a binary in memory, including its address range,
 * file offset, and metadata like build ID
 */
export interface Mapping {
  /** Address at which the binary (or DLL) is loaded into memory. */
  memoryStart: bigint;
  /** The limit of the address range occupied by this mapping. */
  memoryLimit: bigint;
  /** Offset in the binary that corresponds to the first mapped address. */
  fileOffset: bigint;
  /**
   * The object this entry is loaded from.  This can be a filename on
   * disk for the main binary and shared libraries, or virtual
   * abstractions like "[vdso]".
   */
  filenameStrindex: number;
  /** References to attributes in ProfilesDictionary.attribute_table. [optional] */
  attributeIndices: number[];
  /** The following fields indicate the resolution of symbolic info. */
  hasFunctions: boolean;
  hasFilenames: boolean;
  hasLineNumbers: boolean;
  hasInlineFrames: boolean;
}

/** Describes function and line table debug information. */
export interface Location {
  /**
   * Reference to mapping in ProfilesDictionary.mapping_table.
   * It can be unset / set to 0 if the mapping is unknown or not applicable for
   * this profile type, as mapping_table[0] is always a 'null' default mapping.
   */
  mappingIndex: number;
  /**
   * The instruction address for this location, if available.  It
   * should be within [Mapping.memory_start...Mapping.memory_limit]
   * for the corresponding mapping. A non-leaf address may be in the
   * middle of a call instruction. It is up to display tools to find
   * the beginning of the instruction if necessary.
   */
  address: bigint;
  /**
   * Multiple line indicates this location has inlined functions,
   * where the last entry represents the caller into which the
   * preceding entries were inlined.
   *
   * E.g., if memcpy() is inlined into printf:
   *    line[0].function_name == "memcpy"
   *    line[1].function_name == "printf"
   */
  line: Line[];
  /**
   * Provides an indication that multiple symbols map to this location's
   * address, for example due to identical code folding by the linker. In that
   * case the line information above represents one of the multiple
   * symbols. This field must be recomputed when the symbolization state of the
   * profile changes.
   */
  isFolded: boolean;
  /** References to attributes in ProfilesDictionary.attribute_table. [optional] */
  attributeIndices: number[];
}

/** Details a specific line in a source code, linked to a function. */
export interface Line {
  /** Reference to function in ProfilesDictionary.function_table. */
  functionIndex: number;
  /** Line number in source code. 0 means unset. */
  line: bigint;
  /** Column number in source code. 0 means unset. */
  column: bigint;
}

/**
 * Describes a function, including its human-readable name, system name,
 * source file, and starting line number in the source.
 */
export interface FunctionMessage {
  /** Function name. Empty string if not available. */
  nameStrindex: number;
  /**
   * Function name, as identified by the system. For instance,
   * it can be a C++ mangled name. Empty string if not available.
   */
  systemNameStrindex: number;
  /** Source file containing the function. Empty string if not available. */
  filenameStrindex: number;
  /** Line number in source file. 0 means unset. */
  startLine: bigint;
}

function createBaseProfilesDictionary(): ProfilesDictionary {
  return {
    mappingTable: [],
    locationTable: [],
    functionTable: [],
    linkTable: [],
    stringTable: [],
    attributeTable: [],
    attributeUnits: [],
  };
}

export const ProfilesDictionary: MessageFns<ProfilesDictionary> = {
  encode(message: ProfilesDictionary, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.mappingTable) {
      Mapping.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.locationTable) {
      Location.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.functionTable) {
      FunctionMessage.encode(v!, writer.uint32(26).fork()).join();
    }
    for (const v of message.linkTable) {
      Link.encode(v!, writer.uint32(34).fork()).join();
    }
    for (const v of message.stringTable) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.attributeTable) {
      KeyValue.encode(v!, writer.uint32(50).fork()).join();
    }
    for (const v of message.attributeUnits) {
      AttributeUnit.encode(v!, writer.uint32(58).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProfilesDictionary {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProfilesDictionary();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.mappingTable.push(Mapping.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.locationTable.push(Location.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.functionTable.push(FunctionMessage.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.linkTable.push(Link.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.stringTable.push(reader.string());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.attributeTable.push(KeyValue.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.attributeUnits.push(AttributeUnit.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProfilesDictionary {
    return {
      mappingTable: globalThis.Array.isArray(object?.mappingTable)
        ? object.mappingTable.map((e: any) => Mapping.fromJSON(e))
        : [],
      locationTable: globalThis.Array.isArray(object?.locationTable)
        ? object.locationTable.map((e: any) => Location.fromJSON(e))
        : [],
      functionTable: globalThis.Array.isArray(object?.functionTable)
        ? object.functionTable.map((e: any) => FunctionMessage.fromJSON(e))
        : [],
      linkTable: globalThis.Array.isArray(object?.linkTable) ? object.linkTable.map((e: any) => Link.fromJSON(e)) : [],
      stringTable: globalThis.Array.isArray(object?.stringTable)
        ? object.stringTable.map((e: any) => globalThis.String(e))
        : [],
      attributeTable: globalThis.Array.isArray(object?.attributeTable)
        ? object.attributeTable.map((e: any) => KeyValue.fromJSON(e))
        : [],
      attributeUnits: globalThis.Array.isArray(object?.attributeUnits)
        ? object.attributeUnits.map((e: any) => AttributeUnit.fromJSON(e))
        : [],
    };
  },

  toJSON(message: ProfilesDictionary): unknown {
    const obj: any = {};
    if (message.mappingTable?.length) {
      obj.mappingTable = message.mappingTable.map((e) => Mapping.toJSON(e));
    }
    if (message.locationTable?.length) {
      obj.locationTable = message.locationTable.map((e) => Location.toJSON(e));
    }
    if (message.functionTable?.length) {
      obj.functionTable = message.functionTable.map((e) => FunctionMessage.toJSON(e));
    }
    if (message.linkTable?.length) {
      obj.linkTable = message.linkTable.map((e) => Link.toJSON(e));
    }
    if (message.stringTable?.length) {
      obj.stringTable = message.stringTable;
    }
    if (message.attributeTable?.length) {
      obj.attributeTable = message.attributeTable.map((e) => KeyValue.toJSON(e));
    }
    if (message.attributeUnits?.length) {
      obj.attributeUnits = message.attributeUnits.map((e) => AttributeUnit.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ProfilesDictionary>, I>>(base?: I): ProfilesDictionary {
    return ProfilesDictionary.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ProfilesDictionary>, I>>(object: I): ProfilesDictionary {
    const message = createBaseProfilesDictionary();
    message.mappingTable = object.mappingTable?.map((e) => Mapping.fromPartial(e)) || [];
    message.locationTable = object.locationTable?.map((e) => Location.fromPartial(e)) || [];
    message.functionTable = object.functionTable?.map((e) => FunctionMessage.fromPartial(e)) || [];
    message.linkTable = object.linkTable?.map((e) => Link.fromPartial(e)) || [];
    message.stringTable = object.stringTable?.map((e) => e) || [];
    message.attributeTable = object.attributeTable?.map((e) => KeyValue.fromPartial(e)) || [];
    message.attributeUnits = object.attributeUnits?.map((e) => AttributeUnit.fromPartial(e)) || [];
    return message;
  },
};

function createBaseProfilesData(): ProfilesData {
  return { resourceProfiles: [], dictionary: undefined };
}

export const ProfilesData: MessageFns<ProfilesData> = {
  encode(message: ProfilesData, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.resourceProfiles) {
      ResourceProfiles.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.dictionary !== undefined) {
      ProfilesDictionary.encode(message.dictionary, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProfilesData {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProfilesData();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.resourceProfiles.push(ResourceProfiles.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.dictionary = ProfilesDictionary.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProfilesData {
    return {
      resourceProfiles: globalThis.Array.isArray(object?.resourceProfiles)
        ? object.resourceProfiles.map((e: any) => ResourceProfiles.fromJSON(e))
        : [],
      dictionary: isSet(object.dictionary) ? ProfilesDictionary.fromJSON(object.dictionary) : undefined,
    };
  },

  toJSON(message: ProfilesData): unknown {
    const obj: any = {};
    if (message.resourceProfiles?.length) {
      obj.resourceProfiles = message.resourceProfiles.map((e) => ResourceProfiles.toJSON(e));
    }
    if (message.dictionary !== undefined) {
      obj.dictionary = ProfilesDictionary.toJSON(message.dictionary);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ProfilesData>, I>>(base?: I): ProfilesData {
    return ProfilesData.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ProfilesData>, I>>(object: I): ProfilesData {
    const message = createBaseProfilesData();
    message.resourceProfiles = object.resourceProfiles?.map((e) => ResourceProfiles.fromPartial(e)) || [];
    message.dictionary = (object.dictionary !== undefined && object.dictionary !== null)
      ? ProfilesDictionary.fromPartial(object.dictionary)
      : undefined;
    return message;
  },
};

function createBaseResourceProfiles(): ResourceProfiles {
  return { resource: undefined, scopeProfiles: [], schemaUrl: "" };
}

export const ResourceProfiles: MessageFns<ResourceProfiles> = {
  encode(message: ResourceProfiles, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.resource !== undefined) {
      Resource.encode(message.resource, writer.uint32(10).fork()).join();
    }
    for (const v of message.scopeProfiles) {
      ScopeProfiles.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.schemaUrl !== "") {
      writer.uint32(26).string(message.schemaUrl);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResourceProfiles {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResourceProfiles();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.resource = Resource.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.scopeProfiles.push(ScopeProfiles.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.schemaUrl = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ResourceProfiles {
    return {
      resource: isSet(object.resource) ? Resource.fromJSON(object.resource) : undefined,
      scopeProfiles: globalThis.Array.isArray(object?.scopeProfiles)
        ? object.scopeProfiles.map((e: any) => ScopeProfiles.fromJSON(e))
        : [],
      schemaUrl: isSet(object.schemaUrl) ? globalThis.String(object.schemaUrl) : "",
    };
  },

  toJSON(message: ResourceProfiles): unknown {
    const obj: any = {};
    if (message.resource !== undefined) {
      obj.resource = Resource.toJSON(message.resource);
    }
    if (message.scopeProfiles?.length) {
      obj.scopeProfiles = message.scopeProfiles.map((e) => ScopeProfiles.toJSON(e));
    }
    if (message.schemaUrl !== "") {
      obj.schemaUrl = message.schemaUrl;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ResourceProfiles>, I>>(base?: I): ResourceProfiles {
    return ResourceProfiles.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ResourceProfiles>, I>>(object: I): ResourceProfiles {
    const message = createBaseResourceProfiles();
    message.resource = (object.resource !== undefined && object.resource !== null)
      ? Resource.fromPartial(object.resource)
      : undefined;
    message.scopeProfiles = object.scopeProfiles?.map((e) => ScopeProfiles.fromPartial(e)) || [];
    message.schemaUrl = object.schemaUrl ?? "";
    return message;
  },
};

function createBaseScopeProfiles(): ScopeProfiles {
  return { scope: undefined, profiles: [], schemaUrl: "" };
}

export const ScopeProfiles: MessageFns<ScopeProfiles> = {
  encode(message: ScopeProfiles, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.scope !== undefined) {
      InstrumentationScope.encode(message.scope, writer.uint32(10).fork()).join();
    }
    for (const v of message.profiles) {
      Profile.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.schemaUrl !== "") {
      writer.uint32(26).string(message.schemaUrl);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ScopeProfiles {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseScopeProfiles();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.scope = InstrumentationScope.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.profiles.push(Profile.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.schemaUrl = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ScopeProfiles {
    return {
      scope: isSet(object.scope) ? InstrumentationScope.fromJSON(object.scope) : undefined,
      profiles: globalThis.Array.isArray(object?.profiles) ? object.profiles.map((e: any) => Profile.fromJSON(e)) : [],
      schemaUrl: isSet(object.schemaUrl) ? globalThis.String(object.schemaUrl) : "",
    };
  },

  toJSON(message: ScopeProfiles): unknown {
    const obj: any = {};
    if (message.scope !== undefined) {
      obj.scope = InstrumentationScope.toJSON(message.scope);
    }
    if (message.profiles?.length) {
      obj.profiles = message.profiles.map((e) => Profile.toJSON(e));
    }
    if (message.schemaUrl !== "") {
      obj.schemaUrl = message.schemaUrl;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ScopeProfiles>, I>>(base?: I): ScopeProfiles {
    return ScopeProfiles.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ScopeProfiles>, I>>(object: I): ScopeProfiles {
    const message = createBaseScopeProfiles();
    message.scope = (object.scope !== undefined && object.scope !== null)
      ? InstrumentationScope.fromPartial(object.scope)
      : undefined;
    message.profiles = object.profiles?.map((e) => Profile.fromPartial(e)) || [];
    message.schemaUrl = object.schemaUrl ?? "";
    return message;
  },
};

function createBaseProfile(): Profile {
  return {
    sampleType: [],
    sample: [],
    locationIndices: [],
    timeNanos: 0n,
    durationNanos: 0n,
    periodType: undefined,
    period: 0n,
    commentStrindices: [],
    defaultSampleTypeIndex: 0,
    profileId: new Uint8Array(0),
    droppedAttributesCount: 0,
    originalPayloadFormat: "",
    originalPayload: new Uint8Array(0),
    attributeIndices: [],
  };
}

export const Profile: MessageFns<Profile> = {
  encode(message: Profile, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.sampleType) {
      ValueType.encode(v!, writer.uint32(10).fork()).join();
    }
    for (const v of message.sample) {
      Sample.encode(v!, writer.uint32(18).fork()).join();
    }
    writer.uint32(26).fork();
    for (const v of message.locationIndices) {
      writer.int32(v);
    }
    writer.join();
    if (message.timeNanos !== 0n) {
      if (BigInt.asIntN(64, message.timeNanos) !== message.timeNanos) {
        throw new globalThis.Error("value provided for field message.timeNanos of type int64 too large");
      }
      writer.uint32(32).int64(message.timeNanos);
    }
    if (message.durationNanos !== 0n) {
      if (BigInt.asIntN(64, message.durationNanos) !== message.durationNanos) {
        throw new globalThis.Error("value provided for field message.durationNanos of type int64 too large");
      }
      writer.uint32(40).int64(message.durationNanos);
    }
    if (message.periodType !== undefined) {
      ValueType.encode(message.periodType, writer.uint32(50).fork()).join();
    }
    if (message.period !== 0n) {
      if (BigInt.asIntN(64, message.period) !== message.period) {
        throw new globalThis.Error("value provided for field message.period of type int64 too large");
      }
      writer.uint32(56).int64(message.period);
    }
    writer.uint32(66).fork();
    for (const v of message.commentStrindices) {
      writer.int32(v);
    }
    writer.join();
    if (message.defaultSampleTypeIndex !== 0) {
      writer.uint32(72).int32(message.defaultSampleTypeIndex);
    }
    if (message.profileId.length !== 0) {
      writer.uint32(82).bytes(message.profileId);
    }
    if (message.droppedAttributesCount !== 0) {
      writer.uint32(88).uint32(message.droppedAttributesCount);
    }
    if (message.originalPayloadFormat !== "") {
      writer.uint32(98).string(message.originalPayloadFormat);
    }
    if (message.originalPayload.length !== 0) {
      writer.uint32(106).bytes(message.originalPayload);
    }
    writer.uint32(114).fork();
    for (const v of message.attributeIndices) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Profile {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProfile();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.sampleType.push(ValueType.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.sample.push(Sample.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag === 24) {
            message.locationIndices.push(reader.int32());

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.locationIndices.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.timeNanos = reader.int64() as bigint;
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.durationNanos = reader.int64() as bigint;
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.periodType = ValueType.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.period = reader.int64() as bigint;
          continue;
        }
        case 8: {
          if (tag === 64) {
            message.commentStrindices.push(reader.int32());

            continue;
          }

          if (tag === 66) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.commentStrindices.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.defaultSampleTypeIndex = reader.int32();
          continue;
        }
        case 10: {
          if (tag !== 82) {
            break;
          }

          message.profileId = reader.bytes();
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.droppedAttributesCount = reader.uint32();
          continue;
        }
        case 12: {
          if (tag !== 98) {
            break;
          }

          message.originalPayloadFormat = reader.string();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.originalPayload = reader.bytes();
          continue;
        }
        case 14: {
          if (tag === 112) {
            message.attributeIndices.push(reader.int32());

            continue;
          }

          if (tag === 114) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.attributeIndices.push(reader.int32());
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Profile {
    return {
      sampleType: globalThis.Array.isArray(object?.sampleType)
        ? object.sampleType.map((e: any) => ValueType.fromJSON(e))
        : [],
      sample: globalThis.Array.isArray(object?.sample) ? object.sample.map((e: any) => Sample.fromJSON(e)) : [],
      locationIndices: globalThis.Array.isArray(object?.locationIndices)
        ? object.locationIndices.map((e: any) => globalThis.Number(e))
        : [],
      timeNanos: isSet(object.timeNanos) ? BigInt(object.timeNanos) : 0n,
      durationNanos: isSet(object.durationNanos) ? BigInt(object.durationNanos) : 0n,
      periodType: isSet(object.periodType) ? ValueType.fromJSON(object.periodType) : undefined,
      period: isSet(object.period) ? BigInt(object.period) : 0n,
      commentStrindices: globalThis.Array.isArray(object?.commentStrindices)
        ? object.commentStrindices.map((e: any) => globalThis.Number(e))
        : [],
      defaultSampleTypeIndex: isSet(object.defaultSampleTypeIndex)
        ? globalThis.Number(object.defaultSampleTypeIndex)
        : 0,
      profileId: isSet(object.profileId) ? bytesFromBase64(object.profileId) : new Uint8Array(0),
      droppedAttributesCount: isSet(object.droppedAttributesCount)
        ? globalThis.Number(object.droppedAttributesCount)
        : 0,
      originalPayloadFormat: isSet(object.originalPayloadFormat) ? globalThis.String(object.originalPayloadFormat) : "",
      originalPayload: isSet(object.originalPayload) ? bytesFromBase64(object.originalPayload) : new Uint8Array(0),
      attributeIndices: globalThis.Array.isArray(object?.attributeIndices)
        ? object.attributeIndices.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: Profile): unknown {
    const obj: any = {};
    if (message.sampleType?.length) {
      obj.sampleType = message.sampleType.map((e) => ValueType.toJSON(e));
    }
    if (message.sample?.length) {
      obj.sample = message.sample.map((e) => Sample.toJSON(e));
    }
    if (message.locationIndices?.length) {
      obj.locationIndices = message.locationIndices.map((e) => Math.round(e));
    }
    if (message.timeNanos !== 0n) {
      obj.timeNanos = message.timeNanos.toString();
    }
    if (message.durationNanos !== 0n) {
      obj.durationNanos = message.durationNanos.toString();
    }
    if (message.periodType !== undefined) {
      obj.periodType = ValueType.toJSON(message.periodType);
    }
    if (message.period !== 0n) {
      obj.period = message.period.toString();
    }
    if (message.commentStrindices?.length) {
      obj.commentStrindices = message.commentStrindices.map((e) => Math.round(e));
    }
    if (message.defaultSampleTypeIndex !== 0) {
      obj.defaultSampleTypeIndex = Math.round(message.defaultSampleTypeIndex);
    }
    if (message.profileId.length !== 0) {
      obj.profileId = base64FromBytes(message.profileId);
    }
    if (message.droppedAttributesCount !== 0) {
      obj.droppedAttributesCount = Math.round(message.droppedAttributesCount);
    }
    if (message.originalPayloadFormat !== "") {
      obj.originalPayloadFormat = message.originalPayloadFormat;
    }
    if (message.originalPayload.length !== 0) {
      obj.originalPayload = base64FromBytes(message.originalPayload);
    }
    if (message.attributeIndices?.length) {
      obj.attributeIndices = message.attributeIndices.map((e) => Math.round(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Profile>, I>>(base?: I): Profile {
    return Profile.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Profile>, I>>(object: I): Profile {
    const message = createBaseProfile();
    message.sampleType = object.sampleType?.map((e) => ValueType.fromPartial(e)) || [];
    message.sample = object.sample?.map((e) => Sample.fromPartial(e)) || [];
    message.locationIndices = object.locationIndices?.map((e) => e) || [];
    message.timeNanos = object.timeNanos ?? 0n;
    message.durationNanos = object.durationNanos ?? 0n;
    message.periodType = (object.periodType !== undefined && object.periodType !== null)
      ? ValueType.fromPartial(object.periodType)
      : undefined;
    message.period = object.period ?? 0n;
    message.commentStrindices = object.commentStrindices?.map((e) => e) || [];
    message.defaultSampleTypeIndex = object.defaultSampleTypeIndex ?? 0;
    message.profileId = object.profileId ?? new Uint8Array(0);
    message.droppedAttributesCount = object.droppedAttributesCount ?? 0;
    message.originalPayloadFormat = object.originalPayloadFormat ?? "";
    message.originalPayload = object.originalPayload ?? new Uint8Array(0);
    message.attributeIndices = object.attributeIndices?.map((e) => e) || [];
    return message;
  },
};

function createBaseAttributeUnit(): AttributeUnit {
  return { attributeKeyStrindex: 0, unitStrindex: 0 };
}

export const AttributeUnit: MessageFns<AttributeUnit> = {
  encode(message: AttributeUnit, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.attributeKeyStrindex !== 0) {
      writer.uint32(8).int32(message.attributeKeyStrindex);
    }
    if (message.unitStrindex !== 0) {
      writer.uint32(16).int32(message.unitStrindex);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AttributeUnit {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAttributeUnit();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.attributeKeyStrindex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.unitStrindex = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AttributeUnit {
    return {
      attributeKeyStrindex: isSet(object.attributeKeyStrindex) ? globalThis.Number(object.attributeKeyStrindex) : 0,
      unitStrindex: isSet(object.unitStrindex) ? globalThis.Number(object.unitStrindex) : 0,
    };
  },

  toJSON(message: AttributeUnit): unknown {
    const obj: any = {};
    if (message.attributeKeyStrindex !== 0) {
      obj.attributeKeyStrindex = Math.round(message.attributeKeyStrindex);
    }
    if (message.unitStrindex !== 0) {
      obj.unitStrindex = Math.round(message.unitStrindex);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AttributeUnit>, I>>(base?: I): AttributeUnit {
    return AttributeUnit.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AttributeUnit>, I>>(object: I): AttributeUnit {
    const message = createBaseAttributeUnit();
    message.attributeKeyStrindex = object.attributeKeyStrindex ?? 0;
    message.unitStrindex = object.unitStrindex ?? 0;
    return message;
  },
};

function createBaseLink(): Link {
  return { traceId: new Uint8Array(0), spanId: new Uint8Array(0) };
}

export const Link: MessageFns<Link> = {
  encode(message: Link, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.traceId.length !== 0) {
      writer.uint32(10).bytes(message.traceId);
    }
    if (message.spanId.length !== 0) {
      writer.uint32(18).bytes(message.spanId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Link {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLink();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.traceId = reader.bytes();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.spanId = reader.bytes();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Link {
    return {
      traceId: isSet(object.traceId) ? bytesFromBase64(object.traceId) : new Uint8Array(0),
      spanId: isSet(object.spanId) ? bytesFromBase64(object.spanId) : new Uint8Array(0),
    };
  },

  toJSON(message: Link): unknown {
    const obj: any = {};
    if (message.traceId.length !== 0) {
      obj.traceId = base64FromBytes(message.traceId);
    }
    if (message.spanId.length !== 0) {
      obj.spanId = base64FromBytes(message.spanId);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Link>, I>>(base?: I): Link {
    return Link.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Link>, I>>(object: I): Link {
    const message = createBaseLink();
    message.traceId = object.traceId ?? new Uint8Array(0);
    message.spanId = object.spanId ?? new Uint8Array(0);
    return message;
  },
};

function createBaseValueType(): ValueType {
  return { typeStrindex: 0, unitStrindex: 0, aggregationTemporality: 0 };
}

export const ValueType: MessageFns<ValueType> = {
  encode(message: ValueType, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.typeStrindex !== 0) {
      writer.uint32(8).int32(message.typeStrindex);
    }
    if (message.unitStrindex !== 0) {
      writer.uint32(16).int32(message.unitStrindex);
    }
    if (message.aggregationTemporality !== 0) {
      writer.uint32(24).int32(message.aggregationTemporality);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ValueType {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseValueType();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.typeStrindex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.unitStrindex = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.aggregationTemporality = reader.int32() as any;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ValueType {
    return {
      typeStrindex: isSet(object.typeStrindex) ? globalThis.Number(object.typeStrindex) : 0,
      unitStrindex: isSet(object.unitStrindex) ? globalThis.Number(object.unitStrindex) : 0,
      aggregationTemporality: isSet(object.aggregationTemporality)
        ? aggregationTemporalityFromJSON(object.aggregationTemporality)
        : 0,
    };
  },

  toJSON(message: ValueType): unknown {
    const obj: any = {};
    if (message.typeStrindex !== 0) {
      obj.typeStrindex = Math.round(message.typeStrindex);
    }
    if (message.unitStrindex !== 0) {
      obj.unitStrindex = Math.round(message.unitStrindex);
    }
    if (message.aggregationTemporality !== 0) {
      obj.aggregationTemporality = aggregationTemporalityToJSON(message.aggregationTemporality);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ValueType>, I>>(base?: I): ValueType {
    return ValueType.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ValueType>, I>>(object: I): ValueType {
    const message = createBaseValueType();
    message.typeStrindex = object.typeStrindex ?? 0;
    message.unitStrindex = object.unitStrindex ?? 0;
    message.aggregationTemporality = object.aggregationTemporality ?? 0;
    return message;
  },
};

function createBaseSample(): Sample {
  return {
    locationsStartIndex: 0,
    locationsLength: 0,
    value: [],
    attributeIndices: [],
    linkIndex: 0,
    timestampsUnixNano: [],
  };
}

export const Sample: MessageFns<Sample> = {
  encode(message: Sample, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.locationsStartIndex !== 0) {
      writer.uint32(8).int32(message.locationsStartIndex);
    }
    if (message.locationsLength !== 0) {
      writer.uint32(16).int32(message.locationsLength);
    }
    writer.uint32(26).fork();
    for (const v of message.value) {
      if (BigInt.asIntN(64, v) !== v) {
        throw new globalThis.Error("a value provided in array field value of type int64 is too large");
      }
      writer.int64(v);
    }
    writer.join();
    writer.uint32(34).fork();
    for (const v of message.attributeIndices) {
      writer.int32(v);
    }
    writer.join();
    if (message.linkIndex !== 0) {
      writer.uint32(40).int32(message.linkIndex);
    }
    writer.uint32(50).fork();
    for (const v of message.timestampsUnixNano) {
      if (BigInt.asUintN(64, v) !== v) {
        throw new globalThis.Error("a value provided in array field timestampsUnixNano of type uint64 is too large");
      }
      writer.uint64(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Sample {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSample();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.locationsStartIndex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.locationsLength = reader.int32();
          continue;
        }
        case 3: {
          if (tag === 24) {
            message.value.push(reader.int64() as bigint);

            continue;
          }

          if (tag === 26) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.value.push(reader.int64() as bigint);
            }

            continue;
          }

          break;
        }
        case 4: {
          if (tag === 32) {
            message.attributeIndices.push(reader.int32());

            continue;
          }

          if (tag === 34) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.attributeIndices.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.linkIndex = reader.int32();
          continue;
        }
        case 6: {
          if (tag === 48) {
            message.timestampsUnixNano.push(reader.uint64() as bigint);

            continue;
          }

          if (tag === 50) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.timestampsUnixNano.push(reader.uint64() as bigint);
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Sample {
    return {
      locationsStartIndex: isSet(object.locationsStartIndex) ? globalThis.Number(object.locationsStartIndex) : 0,
      locationsLength: isSet(object.locationsLength) ? globalThis.Number(object.locationsLength) : 0,
      value: globalThis.Array.isArray(object?.value) ? object.value.map((e: any) => BigInt(e)) : [],
      attributeIndices: globalThis.Array.isArray(object?.attributeIndices)
        ? object.attributeIndices.map((e: any) => globalThis.Number(e))
        : [],
      linkIndex: isSet(object.linkIndex) ? globalThis.Number(object.linkIndex) : 0,
      timestampsUnixNano: globalThis.Array.isArray(object?.timestampsUnixNano)
        ? object.timestampsUnixNano.map((e: any) => BigInt(e))
        : [],
    };
  },

  toJSON(message: Sample): unknown {
    const obj: any = {};
    if (message.locationsStartIndex !== 0) {
      obj.locationsStartIndex = Math.round(message.locationsStartIndex);
    }
    if (message.locationsLength !== 0) {
      obj.locationsLength = Math.round(message.locationsLength);
    }
    if (message.value?.length) {
      obj.value = message.value.map((e) => e.toString());
    }
    if (message.attributeIndices?.length) {
      obj.attributeIndices = message.attributeIndices.map((e) => Math.round(e));
    }
    if (message.linkIndex !== 0) {
      obj.linkIndex = Math.round(message.linkIndex);
    }
    if (message.timestampsUnixNano?.length) {
      obj.timestampsUnixNano = message.timestampsUnixNano.map((e) => e.toString());
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Sample>, I>>(base?: I): Sample {
    return Sample.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Sample>, I>>(object: I): Sample {
    const message = createBaseSample();
    message.locationsStartIndex = object.locationsStartIndex ?? 0;
    message.locationsLength = object.locationsLength ?? 0;
    message.value = object.value?.map((e) => e) || [];
    message.attributeIndices = object.attributeIndices?.map((e) => e) || [];
    message.linkIndex = object.linkIndex ?? 0;
    message.timestampsUnixNano = object.timestampsUnixNano?.map((e) => e) || [];
    return message;
  },
};

function createBaseMapping(): Mapping {
  return {
    memoryStart: 0n,
    memoryLimit: 0n,
    fileOffset: 0n,
    filenameStrindex: 0,
    attributeIndices: [],
    hasFunctions: false,
    hasFilenames: false,
    hasLineNumbers: false,
    hasInlineFrames: false,
  };
}

export const Mapping: MessageFns<Mapping> = {
  encode(message: Mapping, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.memoryStart !== 0n) {
      if (BigInt.asUintN(64, message.memoryStart) !== message.memoryStart) {
        throw new globalThis.Error("value provided for field message.memoryStart of type uint64 too large");
      }
      writer.uint32(8).uint64(message.memoryStart);
    }
    if (message.memoryLimit !== 0n) {
      if (BigInt.asUintN(64, message.memoryLimit) !== message.memoryLimit) {
        throw new globalThis.Error("value provided for field message.memoryLimit of type uint64 too large");
      }
      writer.uint32(16).uint64(message.memoryLimit);
    }
    if (message.fileOffset !== 0n) {
      if (BigInt.asUintN(64, message.fileOffset) !== message.fileOffset) {
        throw new globalThis.Error("value provided for field message.fileOffset of type uint64 too large");
      }
      writer.uint32(24).uint64(message.fileOffset);
    }
    if (message.filenameStrindex !== 0) {
      writer.uint32(32).int32(message.filenameStrindex);
    }
    writer.uint32(42).fork();
    for (const v of message.attributeIndices) {
      writer.int32(v);
    }
    writer.join();
    if (message.hasFunctions !== false) {
      writer.uint32(48).bool(message.hasFunctions);
    }
    if (message.hasFilenames !== false) {
      writer.uint32(56).bool(message.hasFilenames);
    }
    if (message.hasLineNumbers !== false) {
      writer.uint32(64).bool(message.hasLineNumbers);
    }
    if (message.hasInlineFrames !== false) {
      writer.uint32(72).bool(message.hasInlineFrames);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Mapping {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseMapping();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.memoryStart = reader.uint64() as bigint;
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.memoryLimit = reader.uint64() as bigint;
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.fileOffset = reader.uint64() as bigint;
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.filenameStrindex = reader.int32();
          continue;
        }
        case 5: {
          if (tag === 40) {
            message.attributeIndices.push(reader.int32());

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.attributeIndices.push(reader.int32());
            }

            continue;
          }

          break;
        }
        case 6: {
          if (tag !== 48) {
            break;
          }

          message.hasFunctions = reader.bool();
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.hasFilenames = reader.bool();
          continue;
        }
        case 8: {
          if (tag !== 64) {
            break;
          }

          message.hasLineNumbers = reader.bool();
          continue;
        }
        case 9: {
          if (tag !== 72) {
            break;
          }

          message.hasInlineFrames = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Mapping {
    return {
      memoryStart: isSet(object.memoryStart) ? BigInt(object.memoryStart) : 0n,
      memoryLimit: isSet(object.memoryLimit) ? BigInt(object.memoryLimit) : 0n,
      fileOffset: isSet(object.fileOffset) ? BigInt(object.fileOffset) : 0n,
      filenameStrindex: isSet(object.filenameStrindex) ? globalThis.Number(object.filenameStrindex) : 0,
      attributeIndices: globalThis.Array.isArray(object?.attributeIndices)
        ? object.attributeIndices.map((e: any) => globalThis.Number(e))
        : [],
      hasFunctions: isSet(object.hasFunctions) ? globalThis.Boolean(object.hasFunctions) : false,
      hasFilenames: isSet(object.hasFilenames) ? globalThis.Boolean(object.hasFilenames) : false,
      hasLineNumbers: isSet(object.hasLineNumbers) ? globalThis.Boolean(object.hasLineNumbers) : false,
      hasInlineFrames: isSet(object.hasInlineFrames) ? globalThis.Boolean(object.hasInlineFrames) : false,
    };
  },

  toJSON(message: Mapping): unknown {
    const obj: any = {};
    if (message.memoryStart !== 0n) {
      obj.memoryStart = message.memoryStart.toString();
    }
    if (message.memoryLimit !== 0n) {
      obj.memoryLimit = message.memoryLimit.toString();
    }
    if (message.fileOffset !== 0n) {
      obj.fileOffset = message.fileOffset.toString();
    }
    if (message.filenameStrindex !== 0) {
      obj.filenameStrindex = Math.round(message.filenameStrindex);
    }
    if (message.attributeIndices?.length) {
      obj.attributeIndices = message.attributeIndices.map((e) => Math.round(e));
    }
    if (message.hasFunctions !== false) {
      obj.hasFunctions = message.hasFunctions;
    }
    if (message.hasFilenames !== false) {
      obj.hasFilenames = message.hasFilenames;
    }
    if (message.hasLineNumbers !== false) {
      obj.hasLineNumbers = message.hasLineNumbers;
    }
    if (message.hasInlineFrames !== false) {
      obj.hasInlineFrames = message.hasInlineFrames;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Mapping>, I>>(base?: I): Mapping {
    return Mapping.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Mapping>, I>>(object: I): Mapping {
    const message = createBaseMapping();
    message.memoryStart = object.memoryStart ?? 0n;
    message.memoryLimit = object.memoryLimit ?? 0n;
    message.fileOffset = object.fileOffset ?? 0n;
    message.filenameStrindex = object.filenameStrindex ?? 0;
    message.attributeIndices = object.attributeIndices?.map((e) => e) || [];
    message.hasFunctions = object.hasFunctions ?? false;
    message.hasFilenames = object.hasFilenames ?? false;
    message.hasLineNumbers = object.hasLineNumbers ?? false;
    message.hasInlineFrames = object.hasInlineFrames ?? false;
    return message;
  },
};

function createBaseLocation(): Location {
  return { mappingIndex: 0, address: 0n, line: [], isFolded: false, attributeIndices: [] };
}

export const Location: MessageFns<Location> = {
  encode(message: Location, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.mappingIndex !== 0) {
      writer.uint32(8).int32(message.mappingIndex);
    }
    if (message.address !== 0n) {
      if (BigInt.asUintN(64, message.address) !== message.address) {
        throw new globalThis.Error("value provided for field message.address of type uint64 too large");
      }
      writer.uint32(16).uint64(message.address);
    }
    for (const v of message.line) {
      Line.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.isFolded !== false) {
      writer.uint32(32).bool(message.isFolded);
    }
    writer.uint32(42).fork();
    for (const v of message.attributeIndices) {
      writer.int32(v);
    }
    writer.join();
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Location {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLocation();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.mappingIndex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.address = reader.uint64() as bigint;
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.line.push(Line.decode(reader, reader.uint32()));
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.isFolded = reader.bool();
          continue;
        }
        case 5: {
          if (tag === 40) {
            message.attributeIndices.push(reader.int32());

            continue;
          }

          if (tag === 42) {
            const end2 = reader.uint32() + reader.pos;
            while (reader.pos < end2) {
              message.attributeIndices.push(reader.int32());
            }

            continue;
          }

          break;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Location {
    return {
      mappingIndex: isSet(object.mappingIndex) ? globalThis.Number(object.mappingIndex) : 0,
      address: isSet(object.address) ? BigInt(object.address) : 0n,
      line: globalThis.Array.isArray(object?.line) ? object.line.map((e: any) => Line.fromJSON(e)) : [],
      isFolded: isSet(object.isFolded) ? globalThis.Boolean(object.isFolded) : false,
      attributeIndices: globalThis.Array.isArray(object?.attributeIndices)
        ? object.attributeIndices.map((e: any) => globalThis.Number(e))
        : [],
    };
  },

  toJSON(message: Location): unknown {
    const obj: any = {};
    if (message.mappingIndex !== 0) {
      obj.mappingIndex = Math.round(message.mappingIndex);
    }
    if (message.address !== 0n) {
      obj.address = message.address.toString();
    }
    if (message.line?.length) {
      obj.line = message.line.map((e) => Line.toJSON(e));
    }
    if (message.isFolded !== false) {
      obj.isFolded = message.isFolded;
    }
    if (message.attributeIndices?.length) {
      obj.attributeIndices = message.attributeIndices.map((e) => Math.round(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Location>, I>>(base?: I): Location {
    return Location.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Location>, I>>(object: I): Location {
    const message = createBaseLocation();
    message.mappingIndex = object.mappingIndex ?? 0;
    message.address = object.address ?? 0n;
    message.line = object.line?.map((e) => Line.fromPartial(e)) || [];
    message.isFolded = object.isFolded ?? false;
    message.attributeIndices = object.attributeIndices?.map((e) => e) || [];
    return message;
  },
};

function createBaseLine(): Line {
  return { functionIndex: 0, line: 0n, column: 0n };
}

export const Line: MessageFns<Line> = {
  encode(message: Line, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.functionIndex !== 0) {
      writer.uint32(8).int32(message.functionIndex);
    }
    if (message.line !== 0n) {
      if (BigInt.asIntN(64, message.line) !== message.line) {
        throw new globalThis.Error("value provided for field message.line of type int64 too large");
      }
      writer.uint32(16).int64(message.line);
    }
    if (message.column !== 0n) {
      if (BigInt.asIntN(64, message.column) !== message.column) {
        throw new globalThis.Error("value provided for field message.column of type int64 too large");
      }
      writer.uint32(24).int64(message.column);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Line {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseLine();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.functionIndex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.line = reader.int64() as bigint;
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.column = reader.int64() as bigint;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Line {
    return {
      functionIndex: isSet(object.functionIndex) ? globalThis.Number(object.functionIndex) : 0,
      line: isSet(object.line) ? BigInt(object.line) : 0n,
      column: isSet(object.column) ? BigInt(object.column) : 0n,
    };
  },

  toJSON(message: Line): unknown {
    const obj: any = {};
    if (message.functionIndex !== 0) {
      obj.functionIndex = Math.round(message.functionIndex);
    }
    if (message.line !== 0n) {
      obj.line = message.line.toString();
    }
    if (message.column !== 0n) {
      obj.column = message.column.toString();
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Line>, I>>(base?: I): Line {
    return Line.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Line>, I>>(object: I): Line {
    const message = createBaseLine();
    message.functionIndex = object.functionIndex ?? 0;
    message.line = object.line ?? 0n;
    message.column = object.column ?? 0n;
    return message;
  },
};

function createBaseFunctionMessage(): FunctionMessage {
  return { nameStrindex: 0, systemNameStrindex: 0, filenameStrindex: 0, startLine: 0n };
}

export const FunctionMessage: MessageFns<FunctionMessage> = {
  encode(message: FunctionMessage, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.nameStrindex !== 0) {
      writer.uint32(8).int32(message.nameStrindex);
    }
    if (message.systemNameStrindex !== 0) {
      writer.uint32(16).int32(message.systemNameStrindex);
    }
    if (message.filenameStrindex !== 0) {
      writer.uint32(24).int32(message.filenameStrindex);
    }
    if (message.startLine !== 0n) {
      if (BigInt.asIntN(64, message.startLine) !== message.startLine) {
        throw new globalThis.Error("value provided for field message.startLine of type int64 too large");
      }
      writer.uint32(32).int64(message.startLine);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FunctionMessage {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFunctionMessage();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.nameStrindex = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.systemNameStrindex = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.filenameStrindex = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.startLine = reader.int64() as bigint;
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FunctionMessage {
    return {
      nameStrindex: isSet(object.nameStrindex) ? globalThis.Number(object.nameStrindex) : 0,
      systemNameStrindex: isSet(object.systemNameStrindex) ? globalThis.Number(object.systemNameStrindex) : 0,
      filenameStrindex: isSet(object.filenameStrindex) ? globalThis.Number(object.filenameStrindex) : 0,
      startLine: isSet(object.startLine) ? BigInt(object.startLine) : 0n,
    };
  },

  toJSON(message: FunctionMessage): unknown {
    const obj: any = {};
    if (message.nameStrindex !== 0) {
      obj.nameStrindex = Math.round(message.nameStrindex);
    }
    if (message.systemNameStrindex !== 0) {
      obj.systemNameStrindex = Math.round(message.systemNameStrindex);
    }
    if (message.filenameStrindex !== 0) {
      obj.filenameStrindex = Math.round(message.filenameStrindex);
    }
    if (message.startLine !== 0n) {
      obj.startLine = message.startLine.toString();
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FunctionMessage>, I>>(base?: I): FunctionMessage {
    return FunctionMessage.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FunctionMessage>, I>>(object: I): FunctionMessage {
    const message = createBaseFunctionMessage();
    message.nameStrindex = object.nameStrindex ?? 0;
    message.systemNameStrindex = object.systemNameStrindex ?? 0;
    message.filenameStrindex = object.filenameStrindex ?? 0;
    message.startLine = object.startLine ?? 0n;
    return message;
  },
};

function bytesFromBase64(b64: string): Uint8Array {
  if ((globalThis as any).Buffer) {
    return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
  } else {
    const bin = globalThis.atob(b64);
    const arr = new Uint8Array(bin.length);
    for (let i = 0; i < bin.length; ++i) {
      arr[i] = bin.charCodeAt(i);
    }
    return arr;
  }
}

function base64FromBytes(arr: Uint8Array): string {
  if ((globalThis as any).Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin: string[] = [];
    arr.forEach((byte) => {
      bin.push(globalThis.String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | bigint | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
